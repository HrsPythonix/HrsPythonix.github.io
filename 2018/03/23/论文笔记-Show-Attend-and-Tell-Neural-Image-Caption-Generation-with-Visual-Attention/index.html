<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Image Caption,论文笔记," />










<meta name="description" content="首先先贴一下论文链接，在上一篇笔记中介绍了NIC，它使用的是encoder-decoder框架，使用CNN作为encoder提取特征，将softmax的前一层的vector作为encoder端的输出，然后将其输入进使用LSTM的decoder中进行解码并生成句子。总体来讲模型还是很直观的，而且相对于一般的encoder-decoder框架，只在一开始输入了图像特征，不过它的训练过程应用了很多技巧，">
<meta name="keywords" content="Image Caption,论文笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="论文笔记 : Show, Attend and Tell: Neural Image Caption Generation with Visual Attention">
<meta property="og:url" content="http://yoursite.com/2018/03/23/论文笔记-Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/index.html">
<meta property="og:site_name" content="盗泉">
<meta property="og:description" content="首先先贴一下论文链接，在上一篇笔记中介绍了NIC，它使用的是encoder-decoder框架，使用CNN作为encoder提取特征，将softmax的前一层的vector作为encoder端的输出，然后将其输入进使用LSTM的decoder中进行解码并生成句子。总体来讲模型还是很直观的，而且相对于一般的encoder-decoder框架，只在一开始输入了图像特征，不过它的训练过程应用了很多技巧，">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmuo0hhvwj30xt0iu434.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmv5a8oibj31q10nb7a6.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmv5q40ppj30qe04xaaj.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmv921w10j31eq0fo3zn.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmvc99zsrj305i06qt8k.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmvhx5m8qj31eq0fo3zz.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXly1fpmvlnamk7j31eq0fotaj.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmvoo2dozj31eq0fo0ub.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmvr8e5ebj31eq0foq4r.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmwombycej30go0fawfy.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmzyazbuwj30uw0joh4j.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmzvxfy7jj30sb0lkn4w.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmzwg0ejrj30tb0mk15z.jpg">
<meta property="og:updated_time" content="2018-03-23T11:56:40.672Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="论文笔记 : Show, Attend and Tell: Neural Image Caption Generation with Visual Attention">
<meta name="twitter:description" content="首先先贴一下论文链接，在上一篇笔记中介绍了NIC，它使用的是encoder-decoder框架，使用CNN作为encoder提取特征，将softmax的前一层的vector作为encoder端的输出，然后将其输入进使用LSTM的decoder中进行解码并生成句子。总体来讲模型还是很直观的，而且相对于一般的encoder-decoder框架，只在一开始输入了图像特征，不过它的训练过程应用了很多技巧，">
<meta name="twitter:image" content="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmuo0hhvwj30xt0iu434.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/03/23/论文笔记-Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/"/>





  <title>论文笔记 : Show, Attend and Tell: Neural Image Caption Generation with Visual Attention | 盗泉</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">
  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">盗泉</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一个干净明亮的地方</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tag"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/23/论文笔记-Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pythonix Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="盗泉">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">论文笔记 : Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-23T13:00:59+08:00">
                2018-03-23
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/23/论文笔记-Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/03/23/论文笔记-Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>首先先贴一下<a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="noopener">论文链接</a>，在上一篇笔记中介绍了NIC，它使用的是encoder-decoder框架，使用CNN作为encoder提取特征，将softmax的前一层的vector作为encoder端的输出，然后将其输入进使用LSTM的decoder中进行解码并生成句子。总体来讲模型还是很直观的，而且相对于一般的encoder-decoder框架，只在一开始输入了图像特征，不过它的训练过程应用了很多技巧，大致来讲越简单的模型训练就要越讲究吧。</p>
<h1 id="1-引入Attention机制"><a href="#1-引入Attention机制" class="headerlink" title="1 引入Attention机制"></a>1 引入Attention机制</h1><p>在没有Attention机制的seq2seq任务中，decoder在每个解码时刻接收到的encoder vector都是一样的。但是稍加考虑就能想到，在生成不同的词时，所关注的信息应该是不一样的，例如将I like eating fish翻译成我喜欢吃鱼时，当生成”喜欢”的时候，模型应该更加关注”like”。而Attention机制就是为了实现这一机制，即在生成一个词时，去关注对应的<strong>应关注的显著(salient)信息</strong>，一般就是对输入的信息<strong>各个局部添加权重</strong>来凸显显著信息。而Show, Attend and Tell: Neural Image Caption Generation with Visual这篇论文在Image Caption中引入了Attention机制，属于引入比较早期的论文。</p>
<h1 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2 模型结构"></a>2 模型结构</h1><p>本文的caption模型中，在解码的每个时刻都会接收由attention机制所计算出的编码向量。作者首先指出，由于此前的工作使用的是CNN在Softmax之前的那一层vector表示作为图像特征，这就有一个缺点，就是会丢失能够使caption变得更丰富的一些信息（例如NICv2那篇论文提到在分类数据集上预训练的CNN抛弃了诸如颜色等对分类没有太大帮助的特征），因此这里使用更low-level的表示。随后提出了使用的两种attention：“soft” deterministic attention 和 “hard” stochastic attention，它们通过$\phi$函数来控制。模型的大致结构图如下：</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmuo0hhvwj30xt0iu434.jpg" alt="model-image"></p>
<h2 id="2-1-encoder-part"><a href="#2-1-encoder-part" class="headerlink" title="2.1 encoder part"></a>2.1 encoder part</h2><p>在encoder阶段，照例还是使用CNN来进行提取，具体来讲是提取L个D维的vector，每个vector都对应着图像的一个区域</p>
<p>$$<br>a={ \textbf{a}_1,…,\textbf{a}_L },\quad\textbf{a}_i\in\mathbb{R}^D<br>$$</p>
<p>我们知道，此前的NIC使用Softmax层之前的那一层vector作为图像特征，这可能会丢失一些对于分类无关紧要的特征(例如作者提到的颜色)，而本文的模型是使用来自于 low-level 的卷积层的vector作为图像特征，这可以保留部分特征并且使得decoder可以通过选择所有特征向量的子集来选择性地聚焦于图像的某些部分。</p>
<h2 id="2-2-decoder-part"><a href="#2-2-decoder-part" class="headerlink" title="2.2 decoder part"></a>2.2 decoder part</h2><p>和之前的套路一样，decoder使用RNN(LSTM)来生成词序列。</p>
<p>首先先简略的回顾一下LSTM模型，参考<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">这里</a>：</p>
<h3 id="2-2-1-LSTM-回顾"><a href="#2-2-1-LSTM-回顾" class="headerlink" title="2.2.1 LSTM 回顾"></a>2.2.1 LSTM 回顾</h3><p>首先给出最基本的LSTM框架图：</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmv5a8oibj31q10nb7a6.jpg" alt="LSTM1"></p>
<p>其中图标定义如下图：</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmv5q40ppj30qe04xaaj.jpg" alt="LSTM2"></p>
<p>每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量点乘，黄色的矩阵就是学习到的神经网络层。合并的线表示把向量进行连接，分开的线表示复制向量并分发到不同的位置。</p>
<p>下面来一块一块地解读LSTM细胞。</p>
<h4 id="2-2-1-1-细胞状态传输"><a href="#2-2-1-1-细胞状态传输" class="headerlink" title="2.2.1.1 细胞状态传输"></a>2.2.1.1 细胞状态传输</h4><p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmv921w10j31eq0fo3zn.jpg" alt="LSTM3"></p>
<p>这部分很好理解，就是接收上一个细胞的状态，在这个细胞中进行“选择性遗忘”和“选择性记忆”更新细胞状态然后输出到下一个细胞状态。类似于传送带。直接在整个链上运行，只有一些少量的线性交互。</p>
<p>为了实现“选择性遗忘”和“选择性记忆”，LSTM设计了一个称作为“门”的结构，用来去除或者增加信息到细胞状态中。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。如下图所示：</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmvc99zsrj305i06qt8k.jpg" alt="LSTM4"></p>
<p>Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”。</p>
<p>从上面的大结构容易看出LSTM一共有三个门，来保护和控制细胞状态。</p>
<h4 id="2-2-1-2-选择性遗忘-遗忘门"><a href="#2-2-1-2-选择性遗忘-遗忘门" class="headerlink" title="2.2.1.2 选择性遗忘(遗忘门)"></a>2.2.1.2 选择性遗忘(遗忘门)</h4><p>LSTM 中的第一步是决定我们会从上次传输进来的细胞状态中丢弃什么信息。这个决定通过一个称为忘记门的层实现。该门会读取 $h_{t-1}$(上个细胞的输出) 和 $x_t$(当前细胞的输入)，输出一个在 0 到 1 之间的数值对应每个在细胞状态 $C_{t-1}$ 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmvhx5m8qj31eq0fo3zz.jpg" alt="LSTM5"></p>
<p>举一个语言模型的例子，任务是基于已经看到的预测下一个词。在这个问题中，当前细胞状态可能包含当前主语的性别，因此在生成代词时，正确的代词可以被选择出来。而当我们看到新的主语时，我们就会希望忘记旧的主语。</p>
<h4 id="2-2-1-3-选择性记忆"><a href="#2-2-1-3-选择性记忆" class="headerlink" title="2.2.1.3 选择性记忆"></a>2.2.1.3 选择性记忆</h4><p>下一步是确定什么样的新信息将被添加进细胞状态中。这里包含两个部分。首先，sigmoid 层称 “输入门层” <strong>决定我们将要更新什么值</strong>。然后，一个 tanh 层创建一个新的<strong>候选值向量</strong>，$\tilde{C}_t$，它经过输入门层选择后会被加入到状态中。</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXly1fpmvlnamk7j31eq0fotaj.jpg" alt="LSTM6"><br>例如，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p>
<h4 id="2-2-1-4-更新细胞状态"><a href="#2-2-1-4-更新细胞状态" class="headerlink" title="2.2.1.4 更新细胞状态"></a>2.2.1.4 更新细胞状态</h4><p>现在是更新旧细胞状态的时候了，$C_{t-1}$ 被更新为 $C_t$。前面的步骤已经决定了将会添加什么信息遗忘什么信息，现在就是进行具体的操作实现。</p>
<p>我们把旧状态与 $f_t$ 相乘，就会丢弃掉我们之前决定丢弃的信息。接着加上 $i_t * \tilde{C}_t$就产生了新的细胞状态。</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmvoo2dozj31eq0fo0ub.jpg" alt="LSTM7"></p>
<p>这就是我们实际根据前面确定的，丢弃旧代词的性别信息并添加新的信息的地方。</p>
<h4 id="2-2-1-5-输出信息"><a href="#2-2-1-5-输出信息" class="headerlink" title="2.2.1.5 输出信息"></a>2.2.1.5 输出信息</h4><p>最终，我们需要确定该细胞输出什么值。这个输出是由更新后的细胞状态，当前细胞的输入和上一个细胞的输出决定的。首先，运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去，这部分是由当前细胞的输入和上一个细胞的输出决定的。接着，我们把更新后的细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和上一部 sigmoid 门的输出相乘，最终我们就得到了细胞的输出。</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmvr8e5ebj31eq0foq4r.jpg" alt="LSTM8"></p>
<p>例如，因为模型刚刚看到了一个代词，可能下一个输出就要输出与一个动词相关的信息，可能要输出是否代词是单数还是复数，这样我们就能知道动词是否要进行词形变化。</p>
<h3 id="2-2-2-decoder模型解释"><a href="#2-2-2-decoder模型解释" class="headerlink" title="2.2.2 decoder模型解释"></a>2.2.2 decoder模型解释</h3><p>模型生成的一句caption被表示为各个词的one-hot编码所构成的集合：</p>
<p>$$<br>y={ \textbf{y}_1,…,\textbf{y}_C },\quad\textbf{y}_i\in\mathbb{R}^K<br>$$</p>
<p>其中 K 是词表大小，C 是句子长度。</p>
<p>对应的LSTM的数学模型如下：</p>
<p>$$<br>\begin{pmatrix}\textbf{i}_t \\textbf{f}_t \\textbf{o}_t \\textbf{g}<em>t \\end{pmatrix}=\begin{pmatrix}\sigma\ \sigma\ \sigma\ \tanh \end{pmatrix}T</em>{D+m+n, n} \ \begin{pmatrix}E\textbf{y}<em>{t-1}\ \textbf{h}</em>{t-1}\ \hat{\textbf z}_t \end{pmatrix}<br>$$</p>
<p>$$<br>\textbf{c}_t=\textbf{f}<em>t \odot \textbf{c}</em>{t-1} + \textbf{i}_t \odot \textbf{g}_t<br>$$</p>
<p>$$<br>\textbf{h}_t = \textbf{o}_t \odot \tanh(\textbf{c}_t)<br>$$</p>
<p>第一个式子实际上是四个式子，分别得到输入门、遗忘门、输出门和被输入门控制的候选向量。其中，三个门控由sigmoid激活，得到的是元素值皆在 0 到 1 之间的向量，可以将门控的值视作保留概率(保留量)；候选向量由tanh激活，得到的是元素值皆在 －1 到 1 之间。$T_{s,t}$ 代表的是从 $R^s$ 到 $R^t$ 的仿射变换。</p>
<p>最右边括号里的三个量是四个式子共有的三个输入量：$Ey_{t-1}$ 是look-up得到词 $y_{t-1}$ 的 m 维词向量，这里暂停一下解释一下下Word Embedding，简单来讲就是为了方便把每个单词变成一个向量(通常降维了)，如下图，变换的矩阵叫做embedding matrix就是上面式子里的$E$，注意$E \in R^{m \times K}$，这里的look-up类似于 tensorflow里的tf.nn.embedding_lookup()。</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmwombycej30go0fawfy.jpg" alt="embedding"></p>
<p>继续~<br>$h_{t-1}$是上一时刻的隐状态，即上一个细胞的输出；$\hat{\textbf z}_t\in\mathbb R^D$ 是LSTM真正意义上的“输入”，代表的是捕捉了特定区域视觉信息的上下文向量，它和时刻 t 有关，是一个动态变化的量，在不同的时刻将会捕捉到与本时刻相对应的相关图像区域。而这个量将由attention机制计算得到，后面会详细介绍。相比之下，在NIC模型中，解码过程只在开始输入了图像特征，随后并不存在这个输入，而本文的模型则更像标准的encoder-decoder框架，将encoder端得到的信息在每一时刻都输入decoder。</p>
<p>第二个式子负责更新旧的细胞状态，分为两步，先遗忘后记忆，前面介绍LSTM已经说过，不再赘述。</p>
<p>第三个式子得到隐状态，即细胞的输出。这里作者给出了隐状态和细胞状态的初始值的计算方式，使用两个独立的多层感知机，感知机的输入是各个图像区域特征的平均：</p>
<p>$$<br>\textbf c_0=f_{\text{init,c}}(\frac1L\sum_{i=1}^L\textbf a_i)<br>$$</p>
<p>$$<br>\textbf h_0=f_{\text{init,h}}(\frac1L\sum_{i=1}^L\textbf a_i)<br>$$</p>
<p>有了隐状态，就可以计算词表中各个词的概率值，那么取概率最大的那个作为当前时刻生成的词，并将作为下一时刻的输入。好像就是个全连接层：</p>
<p>$$<br>p(\textbf{y}_t|\textbf{a}, \textbf y_1,…,\textbf y_{t-1}) \propto \exp (L_o (E \textbf{y}_{t-1} + L_h \textbf{h}_t + L_z \hat{\textbf{z}}_t))<br>$$</p>
<p>其中$Lo \in R^{K×m}$， $L_h \in R^{m×n}$， $L_z \in R^{m \times D}$, 和 E 是随机初始化的。</p>
<h3 id="2-2-3-Attention机制"><a href="#2-2-3-Attention机制" class="headerlink" title="2.2.3 Attention机制"></a>2.2.3 Attention机制</h3><p>下面就介绍文中提出的两种Attention机制填上上面的坑。</p>
<p>我们刚刚知道了，通过Attention机制计算出的 $\hat z_t$ 被称为 context vector，是捕捉了特定区域视觉信息的上下文向量。</p>
<p>而Attention要实现的是模型在解码的不同时刻可以关注不同的图像区域，进而可以生成更合理的词。那么，在Attention中就有两个比较关键的量，一个和时刻 t 相关，对应于解码时刻；另一个是输入序列的区域 $\text{a}_i$，对应图像的一个区域。</p>
<p>那么如何实现这一机制呢？</p>
<p>实现这种机制的方式就是在时刻 t ，为输入序列的各个区域 i 计算出一个权重 $\alpha_{ti}$ 。因为输入序列的各个区域的权重的加和为一，所以使用Softmax来实现。至于Softmax需要输入的信息，则需要包含两个方面：一个是被计算的区域 $\text{a}<em>i$，另一个就是上一时刻的信息 $h</em>{t-1}$ ：</p>
<p>$$<br>e_{ti}=f_{\text{att}}(\textbf a_i,\textbf h_{t-1})<br>$$</p>
<p>$$<br>\alpha_{ti}=\frac{\exp(e_{ti})}{\sum_{k=1}^L\exp(e_{tk})}<br>$$</p>
<p>其中$f_{\text{att}}$是结合计算区域 i 和时刻 t 这两个信息的打分函数。论文中使用多层感知机来实现。</p>
<p>得到了权重之后就是计算$\hat{\textbf z}_t$了：</p>
<p>$$<br>\hat{\textbf z}_t=\phi({\textbf a_i},{\alpha_{ti}})<br>$$</p>
<p>这个函数 $\phi $ 就是文中提出的两种attention机制，对应于将权重施加到图像区域到两种不同的策略。下面来分别介绍这两个机制。</p>
<h4 id="2-2-3-1-Stochastic-“Hard”-Attention"><a href="#2-2-3-1-Stochastic-“Hard”-Attention" class="headerlink" title="2.2.3.1 Stochastic “Hard” Attention"></a>2.2.3.1 Stochastic “Hard” Attention</h4><p>首先介绍文中提出的第一种Attention机制，Hard Attention。</p>
<p>在 Hard Attention 机制中，权重 $\alpha_{ti}$ 的意义是是图像区域 $\text{a}<em>i$ 在时刻 t 被选中作为输入decoder的信息的概率，整个图像中有且仅有一个区域会被选中。为此，引入变量 $s</em>{t,i}$ ，当区域 i 被选中时取值为 1 ，否则为 0 。那么就有：</p>
<p>$$<br>\hat{\textbf z}<em>t=\sum</em>{i}s_{t,i}\textbf a_i<br>$$</p>
<p>本文将 $s_t$ 视作隐变量，它是参数是 ${\alpha_i}$ 的多元贝努利分布。</p>
<p>$$<br>p(s_{t,i}=1|s_{j&lt;t},\textbf a)=\alpha_{t,i}<br>$$</p>
<p>为了应用极大似然估计来进行训练，就需要把隐变量边缘化，然后用marginal log-likelihood的下界（variational lower bound）来作为目标函数$L_s$，这就是EM算法的思路：</p>
<p>$$<br>\begin{aligned}\log p(\textbf y|\textbf a)&amp;=\log\sum_sp(s|\textbf a)p(\textbf y|s,\textbf a)\&amp;\geq \sum_sp(s|\textbf a)\log p(\textbf y|s,\textbf a)\end{aligned}<br>$$</p>
<p>$$<br>L_s=\sum_sp(s|\textbf a)\log p(\textbf y|s,\textbf a)<br>$$</p>
<p>其中 y 是为图像 $\text{a}$ 生成的一句caption（由一系列one-hot编码构成的词序列）。不等号那里是Jensen不等式。</p>
<p>接下来对目标函数求梯度：</p>
<p>$$<br>{\partial L_s \over \partial W} = \sum_s p(s | \mathbf{a}) \left[ {\partial \log p(\mathbf{y} | s, \mathbf{a}) \over \partial W} + \log p(\mathbf{y} | s, \mathbf{a}) {\partial \log p(s | \mathbf{a}) \over \partial W} \right]<br>$$</p>
<p>用 N 次蒙特卡洛采样（可以简单理解为抛硬币）来近似：</p>
<p>$$<br>\tilde{s}_t \sim \mathrm{Multinoulli}_L ({ \alpha_i })<br>$$</p>
<p>$$<br>{\partial L_s \over \partial W} \approx {1 \over N} \sum_{n=1}^N p( \tilde{s}^n | \mathbf{a}) \left[ {\partial \log p(\mathbf{y} | \tilde{s}^n, \mathbf{a}) \over \partial W} + \log p(\mathbf{y} | \tilde{s}^n, \mathbf{a}) {\partial \log p(\tilde{s}^n | \mathbf{a}) \over \partial W} \right]<br>$$</p>
<p>另外，在蒙特卡洛方法估计梯度时，可以使用滑动平均来减小梯度的方差。在第 k 个mini-batch时，滑动平均被估计为先前对数似然伴随指数衰减的累积和。</p>
<p>$$<br>b_k=0.9b_{k-1}+0.1\log p(\textbf y|\tilde{s}_k,\textbf a)<br>$$</p>
<p>为进一步减小方差，引入多元贝努利分布的熵 $\mathbb H[s]$ ；而且对于一张给定图片，0.5的概率将 $\tilde{s}$ 设置为它的期望值 $\alpha$ 。这两个技术提升了随机算法的鲁棒性。最终的学习规则为：</p>
<p>$$<br>\begin{aligned}{\partial L_s \over \partial W} \approx {1 \over N} \sum_{n=1}^N p(\tilde{s}^n | \mathbf{a}) &amp;[ {\partial \log p(\mathbf{y} | \tilde{s}^n, \mathbf{a}) \over \partial W}\&amp; + \lambda_r (\log p(\mathbf{y} | \tilde{s}^n, \mathbf{a}) - b) {\partial \log p(\tilde{s}^n | \mathbf{a}) \over \partial W} \&amp;+ \lambda_e {\partial \mathbb H[\tilde{s}^n] \over \partial W} ]\end{aligned}<br>$$</p>
<p>其中的两个系数$\lambda_r \lambda_e$是超参数。实际上，这个规则等价于强化学习规则：attention选择的一系列action的reward是与在采样attention轨迹下目标句子的对数似然成比例的。</p>
<h4 id="2-2-3-2-Deterministic-“Soft”-Attention"><a href="#2-2-3-2-Deterministic-“Soft”-Attention" class="headerlink" title="2.2.3.2 Deterministic “Soft” Attention"></a>2.2.3.2 Deterministic “Soft” Attention</h4><p>在 soft attention 机制中，权重 $\alpha_{ti}$ 所扮演的角色是图像区域 $\text{a}_i$ 在时刻 t 的输入decoder的信息中的所占的比例。既然这样，就将各区域 ai 与对应的权重 αt,i 做加权求和就可以得到 $\hat{z}_t$：</p>
<p>$$<br>\mathbb E_{p(s_t|a)}[\hat{\textbf z}<em>t]=\sum</em>{i=1}^L\alpha_{t,i}\textbf a_i<br>$$</p>
<p>整个模型光滑、可微，利用反向传播来进行end-to-end的训练。</p>
<p>论文里还介绍了更多细节，比如分析了解码过程的一些情况；强迫使图像的每个区域在整个解码过程中的权重之和都相等（这种做法在目标函数中体现为一个惩罚项），来使图像的各个区域都对生成caption起到贡献；增加一个跟时间有关的维度因子可以让模型的注意力集中在图像的目标上。</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmzyazbuwj30uw0joh4j.jpg" alt="compare"></p>
<h2 id="2-3-实验结果"><a href="#2-3-实验结果" class="headerlink" title="2.3 实验结果"></a>2.3 实验结果</h2><p>从结果上看，当使用BLEU作为指标时，几个数据集上都是hard attention的效果好过soft attention；而当使用Meteor作为指标时，情况则相反。</p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmzvxfy7jj30sb0lkn4w.jpg" alt="result1"></p>
<p><img src="https://ws1.sinaimg.cn/large/005E0CcXgy1fpmzwg0ejrj30tb0mk15z.jpg" alt="result2"></p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>请我吃辣条吧~</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/WechatPay.png" alt="Pythonix Huang 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/Alipay.png" alt="Pythonix Huang 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Image-Caption/" rel="tag"># Image Caption</a>
          
            <a href="/tags/论文笔记/" rel="tag"># 论文笔记</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/22/论文笔记-Show-and-Tell-Lessons-learned-from-the-2015-MSCOCO-Image-Captioning-Challenge/" rel="next" title="论文笔记 : Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge">
                <i class="fa fa-chevron-left"></i> 论文笔记 : Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/1.png"
                alt="Pythonix Huang" />
            
              <p class="site-author-name" itemprop="name">Pythonix Huang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/HrsPythonix" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-引入Attention机制"><span class="nav-text">1 引入Attention机制</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-模型结构"><span class="nav-text">2 模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-encoder-part"><span class="nav-text">2.1 encoder part</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-decoder-part"><span class="nav-text">2.2 decoder part</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-LSTM-回顾"><span class="nav-text">2.2.1 LSTM 回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-1-细胞状态传输"><span class="nav-text">2.2.1.1 细胞状态传输</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-2-选择性遗忘-遗忘门"><span class="nav-text">2.2.1.2 选择性遗忘(遗忘门)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-3-选择性记忆"><span class="nav-text">2.2.1.3 选择性记忆</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-4-更新细胞状态"><span class="nav-text">2.2.1.4 更新细胞状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-5-输出信息"><span class="nav-text">2.2.1.5 输出信息</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-decoder模型解释"><span class="nav-text">2.2.2 decoder模型解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-Attention机制"><span class="nav-text">2.2.3 Attention机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-1-Stochastic-“Hard”-Attention"><span class="nav-text">2.2.3.1 Stochastic “Hard” Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-2-Deterministic-“Soft”-Attention"><span class="nav-text">2.2.3.2 Deterministic “Soft” Attention</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-实验结果"><span class="nav-text">2.3 实验结果</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Pythonix Huang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://Pythonix.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/03/23/论文笔记-Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/';
          this.page.identifier = '2018/03/23/论文笔记-Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/';
          this.page.title = '论文笔记 : Show, Attend and Tell: Neural Image Caption Generation with Visual Attention';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://Pythonix.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  <script src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script>
</body>
</html>
